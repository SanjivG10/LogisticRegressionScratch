{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "val = []\n",
    "for eachElem in list(zip(X,y)):\n",
    "    val.append(eachElem)\n",
    "np.random.shuffle(val)\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "for eachElem in val:\n",
    "    X.append(eachElem[0])\n",
    "    y.append(eachElem[1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = np.expand_dims(X,axis=2)\n",
    "#let me change y into categorical \n",
    "y_new = []\n",
    "for eachY in y:\n",
    "    if eachY==0:\n",
    "        y_new.append([1,0,0])\n",
    "    elif eachY==1:\n",
    "        y_new.append([0,1,0])\n",
    "    else:\n",
    "        y_new.append([0,0,1])\n",
    "y = np.array(y_new)\n",
    "y = np.expand_dims(y,axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x,m,b):\n",
    "    return np.dot(m,x)+b\n",
    "\n",
    "def sigmoid(x,derivative=False):\n",
    "    if derivative:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def loss(y_pred,y_actual):\n",
    "    return - np.sum((y_actual*np.log(y_pred) + (1-y_actual)*np.log(1-y_pred)))\n",
    "    \n",
    "\n",
    "def gradientDescent(X,y,learning_rate=0.01,epochs=100):\n",
    "    m = np.random.uniform(0,1,(y[0].shape[0],X[0].shape[0]))\n",
    "    b  = np.zeros((y[0].shape[0],1))\n",
    "    for i in range(epochs):\n",
    "        for eachX,eachY in list(zip(X,y)):\n",
    "            predicted_y_actual = forward_prop(eachX,m,b)\n",
    "            predicted_y = sigmoid(predicted_y_actual)\n",
    "            m_gradient,b_gradient = backprop(eachX,eachY,predicted_y,predicted_y_actual)\n",
    "            m = m-learning_rate*m_gradient\n",
    "            b = b-learning_rate*b_gradient\n",
    "        print('Loss is {}'.format(loss(predicted_y,eachY)))\n",
    "        \n",
    "        \n",
    "\n",
    "def backprop(x,y,y_hat,y_pred_actual):\n",
    "    loss_derivative = -1*y/y_hat + (1-y)/(1-y_hat)\n",
    "    sigmoid_derv = sigmoid(y_pred_actual,True)\n",
    "    b_gradient = loss_derivative*sigmoid_derv\n",
    "    #b_gradient can be replaced by y_pred-y\n",
    "    #it gives same result!!! I fucking love math! \n",
    "    m_gradient  = np.dot(b_gradient,x.T)\n",
    "    return m_gradient,b_gradient\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.768028850247058\n",
      "Loss is 0.5446575584035123\n",
      "Loss is 0.4372785927343956\n",
      "Loss is 0.3743020871906606\n",
      "Loss is 0.33295496870996527\n",
      "Loss is 0.3036376694380258\n",
      "Loss is 0.28163320439935996\n",
      "Loss is 0.26437386864949164\n",
      "Loss is 0.2503544140938534\n",
      "Loss is 0.23864134087216723\n",
      "Loss is 0.2286292250273592\n",
      "Loss is 0.2199105513847873\n",
      "Loss is 0.21220211045463636\n",
      "Loss is 0.2053014530913186\n",
      "Loss is 0.19906015637027993\n",
      "Loss is 0.19336688958488887\n",
      "Loss is 0.18813639253174833\n",
      "Loss is 0.18330212608960025\n",
      "Loss is 0.17881126254828195\n",
      "Loss is 0.174621200986695\n",
      "Loss is 0.17069709751855647\n",
      "Loss is 0.1670100839830331\n",
      "Loss is 0.16353596208771826\n",
      "Loss is 0.16025423146958753\n",
      "Loss is 0.157147356002907\n",
      "Loss is 0.15420020263320655\n",
      "Loss is 0.1513996068960719\n",
      "Loss is 0.14873403268012025\n",
      "Loss is 0.14619330296057653\n",
      "Loss is 0.14376838458941946\n",
      "Loss is 0.1414512146993503\n",
      "Loss is 0.13923455946280958\n",
      "Loss is 0.1371118982421208\n",
      "Loss is 0.13507732783990448\n",
      "Loss is 0.13312548279193964\n",
      "Loss is 0.13125146856268247\n",
      "Loss is 0.12945080519373686\n",
      "Loss is 0.12771937947905657\n",
      "Loss is 0.12605340414108193\n",
      "Loss is 0.12444938279081383\n",
      "Loss is 0.12290407969472315\n",
      "Loss is 0.12141449355912384\n",
      "Loss is 0.11997783469046999\n",
      "Loss is 0.11859150500726424\n",
      "Loss is 0.11725308047271184\n",
      "Loss is 0.11596029559222064\n",
      "Loss is 0.11471102968028882\n",
      "Loss is 0.11350329465032083\n",
      "Loss is 0.11233522412081826\n",
      "Loss is 0.11120506366407065\n",
      "Loss is 0.11011116205033193\n",
      "Loss is 0.10905196336266425\n",
      "Loss is 0.10802599987603774\n",
      "Loss is 0.10703188560961133\n",
      "Loss is 0.10606831047394802\n",
      "Loss is 0.1051340349456885\n",
      "Loss is 0.10422788521128769\n",
      "Loss is 0.10334874872910609\n",
      "Loss is 0.10249557016565486\n",
      "Loss is 0.10166734766738195\n",
      "Loss is 0.10086312943410577\n",
      "Loss is 0.1000820105643157\n",
      "Loss is 0.09932313014603913\n",
      "Loss is 0.09858566857003569\n",
      "Loss is 0.09786884504467584\n",
      "Loss is 0.09717191529417166\n",
      "Loss is 0.09649416942379613\n",
      "Loss is 0.09583492993746975\n",
      "Loss is 0.0951935498946219\n",
      "Loss is 0.09456941119456519\n",
      "Loss is 0.09396192297780165\n",
      "Loss is 0.09337052013472182\n",
      "Loss is 0.09279466191307964\n",
      "Loss is 0.09223383061643668\n",
      "Loss is 0.09168753038651058\n",
      "Loss is 0.09115528606299421\n",
      "Loss is 0.09063664211500498\n",
      "Loss is 0.09013116163883364\n",
      "Loss is 0.08963842541712982\n",
      "Loss is 0.08915803103508155\n",
      "Loss is 0.08868959204951418\n",
      "Loss is 0.08823273720718772\n",
      "Loss is 0.08778710970885863\n",
      "Loss is 0.08735236651597261\n",
      "Loss is 0.08692817769707685\n",
      "Loss is 0.0865142258113074\n",
      "Loss is 0.0861102053264709\n",
      "Loss is 0.08571582206946629\n",
      "Loss is 0.08533079270694148\n",
      "Loss is 0.08495484425424359\n",
      "Loss is 0.08458771361087168\n",
      "Loss is 0.08422914712076117\n",
      "Loss is 0.0838789001558548\n",
      "Loss is 0.08353673672153121\n",
      "Loss is 0.08320242908255054\n",
      "Loss is 0.08287575740828444\n",
      "Loss is 0.08255650943607058\n",
      "Loss is 0.08224448015162301\n",
      "Loss is 0.08193947148549044\n",
      "Loss is 0.08164129202463108\n"
     ]
    }
   ],
   "source": [
    "gradientDescent(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I will try running gradient ascent instead of gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999800002556964\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#let me check if my gradient of a function is correct or not!\n",
    "#doing this will check our gradient of the function that we have calculated! \n",
    "y_hat= 0.5\n",
    "y = 1\n",
    "def getResult(y,y_hat,derv=False):\n",
    "    epsilon = 0.00001\n",
    "    if derv:\n",
    "        return ((y*np.log(y_hat+epsilon)+(1-y)*np.log(1-y_hat-epsilon))-y*np.log(y_hat)+(1-y)*np.log(1-y_hat))/epsilon\n",
    "    return y*np.log(y_hat)+(1-y)*np.log(1-y_hat)\n",
    "gradientOfThatFunction = y/y_hat - (1-y)/(1-y_hat)\n",
    "\n",
    "print(getResult(y,y_hat,True))\n",
    "print(gradientOfThatFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
